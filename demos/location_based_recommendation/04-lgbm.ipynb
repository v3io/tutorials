{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import dok_matrix, coo_matrix\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models/')\n",
    "RANDOM_STATE = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisles = pd.read_csv(os.path.join(DATA_DIR, 'aisles.csv'), dtype={'aisle_id': np.uint8, 'aisle': 'category'})\n",
    "departments = pd.read_csv(DATA_DIR + 'departments.csv', dtype={'department_id': np.uint8,\n",
    "                                                               'department': 'category'})\n",
    "order_prior = pd.read_csv(DATA_DIR + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n",
    "                                                                         'product_id': np.uint16,\n",
    "                                                                         'add_to_cart_order': np.uint8,\n",
    "                                                                         'reordered': bool})\n",
    "order_train = pd.read_csv(DATA_DIR + 'order_products__train.csv', dtype={'order_id': np.uint32,\n",
    "                                                                         'product_id': np.uint16,\n",
    "                                                                         'add_to_cart_order': np.uint8,\n",
    "                                                                         'reordered': bool})\n",
    "orders = pd.read_csv(DATA_DIR + 'orders.csv', dtype={'order_id': np.uint32,\n",
    "                                                     'user_id': np.uint32,\n",
    "                                                     'eval_set': 'category',\n",
    "                                                     'order_number': np.uint8,\n",
    "                                                     'order_dow': np.uint8,\n",
    "                                                     'order_hour_of_day': np.uint8})\n",
    "products = pd.read_csv(DATA_DIR + 'products.csv', dtype={'product_id': np.uint16,\n",
    "                                                         'aisle_id': np.uint8,\n",
    "                                                         'department_id': np.uint8})\n",
    "product_embeddings = pd.read_pickle(DATA_DIR + 'product_embeddings.pkl')\n",
    "\n",
    "product_periods = pd.read_pickle(DATA_DIR + 'product_periods_stat.pkl').fillna(9999)\n",
    "\n",
    "embedings = list(range(32))\n",
    "product_embeddings = product_embeddings[embedings + ['product_id']]\n",
    "\n",
    "user_dep_stat = pd.read_pickle(DATA_DIR + 'user_department_products.pkl')\n",
    "user_aisle_stat = pd.read_pickle(DATA_DIR + 'user_aisle_products.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weights and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create weights\n",
      "creating probabilities\n",
      "Index(['product_id', 'reorder_prob'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>reorder_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.385475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.102564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.486486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.351648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  reorder_prob\n",
       "0           1      0.385475\n",
       "1           2      0.102564\n",
       "2           3      0.486486\n",
       "3           4      0.351648\n",
       "4           5      0.666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Create weights')\n",
    "weights = order_train.groupby('order_id')['reordered'].sum().to_frame('weights')\n",
    "weights.reset_index(inplace=True)\n",
    "\n",
    "print('creating probabilities')\n",
    "prob = pd.merge(order_prior, orders, on='order_id')\n",
    "\n",
    "prob = prob.groupby(['product_id', 'user_id']) \\\n",
    "    .agg({'reordered': 'sum',\n",
    "          'user_id': 'size'})\n",
    "\n",
    "prob.rename(columns={'sum': 'reordered',\n",
    "                     'user_id': 'total'}, inplace=True)\n",
    "\n",
    "prob.reordered = (prob.reordered > 0).astype(np.float32)\n",
    "prob.total = (prob.total > 0).astype(np.float32)\n",
    "prob['reorder_prob'] = prob.reordered / prob.total\n",
    "prob = prob.groupby('product_id').agg({'reorder_prob': 'mean'}).rename(columns={'mean': 'reorder_prob'}) \\\n",
    "    .reset_index()\n",
    "print(prob.columns)\n",
    "prob.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create product stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('creating prod_stat')\n",
    "prod_stat = order_prior.groupby('product_id').agg({'reordered': ['sum', 'size'],\n",
    "                                                   'add_to_cart_order': 'mean'})\n",
    "prod_stat.columns = prod_stat.columns.levels[1]\n",
    "prod_stat.rename(columns={'sum': 'prod_reorders',\n",
    "                          'size': 'prod_orders',\n",
    "                          'mean': 'prod_add_to_card_mean'}, inplace=True)\n",
    "prod_stat.reset_index(inplace=True)\n",
    "\n",
    "prod_stat['reorder_ration'] = prod_stat['prod_reorders'] / prod_stat['prod_reorders']\n",
    "\n",
    "prod_stat = pd.merge(prod_stat, prob, on='product_id')\n",
    "print(prod_stat.columns)\n",
    "prod_stat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('creating user_stat')\n",
    "user_stat = orders.loc[orders.eval_set == 'prior', :].groupby('user_id').agg({'order_number': 'max',\n",
    "                                                                              'days_since_prior_order': ['sum',\n",
    "                                                                                                         'mean',\n",
    "                                                                                                         'median']})\n",
    "user_stat.columns = user_stat.columns.droplevel(0)\n",
    "user_stat.rename(columns={'max': 'user_orders',\n",
    "                          'sum': 'user_order_starts_at',\n",
    "                          'mean': 'user_mean_days_since_prior_order',\n",
    "                          'median': 'user_median_days_since_prior'}, inplace=True)\n",
    "user_stat.reset_index(inplace=True)\n",
    "\n",
    "orders_products = pd.merge(orders, order_prior, on='order_id')\n",
    "\n",
    "user_order_stat = orders_products.groupby('user_id').agg({'user_id': 'size',\n",
    "                                                          'reordered': 'sum',\n",
    "                                                          'product_id': lambda x: x.nunique()})\n",
    "user_order_stat.rename(columns={'user_id': 'user_total_products',\n",
    "                                'product_id': 'user_distinct_products',\n",
    "                                'reordered': 'user_reorder_ratio'}, inplace=True)\n",
    "user_order_stat.reset_index(inplace=True)\n",
    "user_order_stat.user_reorder_ratio = user_order_stat.user_reorder_ratio / user_order_stat.user_total_products\n",
    "\n",
    "user_stat = pd.merge(user_stat, user_order_stat, on='user_id')\n",
    "user_stat['user_average_basket'] = user_stat.user_total_products / user_stat.user_orders\n",
    "print(user_stat.columns)\n",
    "print(user_stat.head(5))\n",
    "print('user order stat')\n",
    "print(user_order_stat.columns)\n",
    "user_order.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create product-user stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating product user\n",
      "Index(['product_id', 'prod_users_unq'], dtype='object')\n",
      "creating product user reordered\n",
      "Index(['product_id', 'prod_users_unq_reordered'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('creating product user')\n",
    "prod_usr = orders_products.groupby(['product_id']).agg({'user_id': lambda x: x.nunique()})\n",
    "prod_usr.rename(columns={'user_id': 'prod_users_unq'}, inplace=True)\n",
    "prod_usr.reset_index(inplace=True)\n",
    "print(prod_usr.columns)\n",
    "\n",
    "print('creating product user reordered')\n",
    "prod_usr_reordered = orders_products.loc[orders_products.reordered, :].groupby(['product_id']).agg(\n",
    "    {'user_id': lambda x: x.nunique()})\n",
    "prod_usr_reordered.rename(columns={'user_id': 'prod_users_unq_reordered'}, inplace=True)\n",
    "prod_usr_reordered.reset_index(inplace=True)\n",
    "print(prod_usr_reordered.columns)\n",
    "\n",
    "order_stat = orders_products.groupby('order_id').agg({'order_id': 'size'}) \\\n",
    "    .rename(columns={'order_id': 'order_size'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create order-product stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating order products\n",
      "Index(['order_id', 'user_id', 'eval_set', 'order_number', 'order_dow',\n",
      "       'order_hour_of_day', 'days_since_prior_order', 'product_id',\n",
      "       'add_to_cart_order', 'reordered', 'order_size',\n",
      "       'add_to_cart_order_inverted', 'add_to_cart_order_relative'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('creating order products')\n",
    "orders_products = pd.merge(orders_products, order_stat, on='order_id')\n",
    "orders_products['add_to_cart_order_inverted'] = orders_products.order_size - orders_products.add_to_cart_order\n",
    "orders_products['add_to_cart_order_relative'] = orders_products.add_to_cart_order / orders_products.order_size\n",
    "print(orders_products.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create day-of-week stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating data_dow\n",
      "Index(['user_id', 'product_id', 'order_dow', 'reordered_dow',\n",
      "       'reordered_dow_size', 'reordered_dow_ration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('creating data_dow')\n",
    "data_dow = orders_products.groupby(['user_id', 'product_id', 'order_dow']).agg({'reordered': ['sum', 'size']})\n",
    "data_dow.columns = data_dow.columns.droplevel(0)\n",
    "data_dow.columns = ['reordered_dow', 'reordered_dow_size']\n",
    "data_dow['reordered_dow_ration'] = data_dow.reordered_dow / data_dow.reordered_dow_size\n",
    "data_dow.reset_index(inplace=True)\n",
    "print(data_dow.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating data\n",
      "Index(['user_id', 'product_id', 'up_orders', 'up_first_order', 'up_last_order',\n",
      "       'up_mean_cart_position', 'up_median_cart_position',\n",
      "       'days_since_prior_order_mean', 'days_since_prior_order_median',\n",
      "       'order_dow_mean', 'order_dow_median', 'order_hour_of_day_mean',\n",
      "       'order_hour_of_day_median', 'add_to_cart_order_inverted_mean',\n",
      "       'add_to_cart_order_inverted_median', 'add_to_cart_order_relative_mean',\n",
      "       'add_to_cart_order_relative_median', 'reordered_sum',\n",
      "       'user_product_reordered_ratio', 'prod_add_to_card_mean', 'prod_orders',\n",
      "       'prod_reorders', 'reorder_ration', 'reorder_prob', 'user_orders',\n",
      "       'user_order_starts_at', 'user_mean_days_since_prior_order',\n",
      "       'user_median_days_since_prior', 'user_total_products',\n",
      "       'user_reorder_ratio', 'user_distinct_products', 'user_average_basket',\n",
      "       'up_order_rate', 'up_orders_since_last_order',\n",
      "       'up_order_rate_since_first_order'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('creating data')\n",
    "data = orders_products.groupby(['user_id', 'product_id']).agg({'user_id': 'size',\n",
    "                                                               'order_number': ['min', 'max'],\n",
    "                                                               'add_to_cart_order': ['mean', 'median'],\n",
    "                                                               'days_since_prior_order': ['mean', 'median'],\n",
    "                                                               'order_dow': ['mean', 'median'],\n",
    "                                                               'order_hour_of_day': ['mean', 'median'],\n",
    "                                                               'add_to_cart_order_inverted': ['mean', 'median'],\n",
    "                                                               'add_to_cart_order_relative': ['mean', 'median'],\n",
    "                                                               'reordered': ['sum']})\n",
    "# data.columns = data.columns.droplevel(0)\n",
    "data.columns = ['up_orders', 'up_first_order', 'up_last_order',\n",
    "                'up_mean_cart_position', 'up_median_cart_position',\n",
    "                'days_since_prior_order_mean', 'days_since_prior_order_median',\n",
    "                'order_dow_mean', 'order_dow_median',\n",
    "                'order_hour_of_day_mean', 'order_hour_of_day_median',\n",
    "                'add_to_cart_order_inverted_mean', 'add_to_cart_order_inverted_median',\n",
    "                'add_to_cart_order_relative_mean', 'add_to_cart_order_relative_median',\n",
    "                'reordered_sum']\n",
    "data['user_product_reordered_ratio'] = (data.reordered_sum + 1.0) / data.up_orders\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "data = pd.merge(data, prod_stat, on='product_id')\n",
    "data = pd.merge(data, user_stat, on='user_id')\n",
    "\n",
    "data['up_order_rate'] = data.up_orders / data.user_orders\n",
    "data['up_orders_since_last_order'] = data.user_orders - data.up_last_order\n",
    "data['up_order_rate_since_first_order'] = data.user_orders / (data.user_orders - data.up_first_order + 1)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating order_train\n",
      "Index(['user_id', 'product_id', 'up_orders', 'up_first_order', 'up_last_order',\n",
      "       'up_mean_cart_position', 'up_median_cart_position',\n",
      "       'days_since_prior_order_mean', 'days_since_prior_order_median',\n",
      "       'order_dow_mean', 'order_dow_median', 'order_hour_of_day_mean',\n",
      "       'order_hour_of_day_median', 'add_to_cart_order_inverted_mean',\n",
      "       'add_to_cart_order_inverted_median', 'add_to_cart_order_relative_mean',\n",
      "       'add_to_cart_order_relative_median', 'reordered_sum',\n",
      "       'user_product_reordered_ratio', 'prod_add_to_card_mean', 'prod_orders',\n",
      "       'prod_reorders', 'reorder_ration', 'reorder_prob', 'user_orders',\n",
      "       'user_order_starts_at', 'user_mean_days_since_prior_order',\n",
      "       'user_median_days_since_prior', 'user_total_products',\n",
      "       'user_reorder_ratio', 'user_distinct_products', 'user_average_basket',\n",
      "       'up_order_rate', 'up_orders_since_last_order',\n",
      "       'up_order_rate_since_first_order'],\n",
      "      dtype='object')\n",
      "data is joined\n"
     ]
    }
   ],
   "source": [
    "print('creating order_train')\n",
    "order_train = pd.merge(order_train, products, on='product_id')\n",
    "order_train = pd.merge(order_train, orders, on='order_id')\n",
    "order_train = pd.merge(order_train, user_dep_stat, on=['user_id', 'department_id'])\n",
    "order_train = pd.merge(order_train, user_aisle_stat, on=['user_id', 'aisle_id'])\n",
    "\n",
    "order_train = pd.merge(order_train, prod_usr, on='product_id')\n",
    "order_train = pd.merge(order_train, prod_usr_reordered, on='product_id', how='left')\n",
    "order_train.prod_users_unq_reordered.fillna(0, inplace=True)\n",
    "\n",
    "order_train = pd.merge(order_train, data, on=['product_id', 'user_id'])\n",
    "order_train = pd.merge(order_train, data_dow, on=['product_id', 'user_id', 'order_dow'], how='left')\n",
    "\n",
    "order_train['aisle_reordered_ratio'] = order_train.aisle_reordered / order_train.user_orders\n",
    "order_train['dep_reordered_ratio'] = order_train.dep_reordered / order_train.user_orders\n",
    "\n",
    "order_train = pd.merge(order_train, product_periods, on=['user_id', 'product_id'])\n",
    "order_train = pd.merge(order_train, product_embeddings, on=['product_id'])\n",
    "print(data.columns)\n",
    "print('data is joined')\n",
    "\n",
    "unique_orders = np.unique(order_train.order_id)\n",
    "orders_train, orders_test = train_test_split(unique_orders, test_size=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "order_test = order_train.loc[np.in1d(order_train.order_id, orders_test)]\n",
    "order_train = order_train.loc[np.in1d(order_train.order_id, orders_train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features and finalize train / validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories:  ['product_id', 'aisle_id', 'department_id', 'user_id', 'order_id']\n",
      "cat features: [(74, 'product_id'), (75, 'aisle_id'), (76, 'department_id'), (77, 'user_id'), (78, 'order_id')]\n",
      "cat str features: 74,75,76,77,78\n",
      "not included: {'product_name', 'add_to_cart_order', 'order_hour_of_day_median', 'user_median_days_since_prior', 'up_median_cart_position', 'reordered_dow_ration', 'add_to_cart_order_relative_median', 'days_since_prior_order_median', 'reordered', 'eval_set', 'reordered_dow_size', 'add_to_cart_order_inverted_median', 'order_dow_median', 'order_hour_of_day_mean', 'reordered_dow'}\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    # 'reordered_dow_ration', 'reordered_dow', 'reordered_dow_size',\n",
    "    # 'reordered_prev', 'add_to_cart_order_prev', 'order_dow_prev', 'order_hour_of_day_prev',\n",
    "    'user_product_reordered_ratio', 'reordered_sum',\n",
    "    'add_to_cart_order_inverted_mean', 'add_to_cart_order_relative_mean',\n",
    "    'reorder_prob',\n",
    "    'last', 'prev1', 'prev2', 'median', 'mean',\n",
    "    'dep_reordered_ratio', 'aisle_reordered_ratio',\n",
    "    'aisle_products',\n",
    "    'aisle_reordered',\n",
    "    'dep_products',\n",
    "    'dep_reordered',\n",
    "    'prod_users_unq', 'prod_users_unq_reordered',\n",
    "    'order_number', 'prod_add_to_card_mean',\n",
    "    'days_since_prior_order',\n",
    "    'order_dow', 'order_hour_of_day',\n",
    "    'reorder_ration',\n",
    "    'user_orders', 'user_order_starts_at', 'user_mean_days_since_prior_order',\n",
    "    # 'user_median_days_since_prior',\n",
    "    'user_average_basket', 'user_distinct_products', 'user_reorder_ratio', 'user_total_products',\n",
    "    'prod_orders', 'prod_reorders',\n",
    "    'up_order_rate', 'up_orders_since_last_order', 'up_order_rate_since_first_order',\n",
    "    'up_orders', 'up_first_order', 'up_last_order', 'up_mean_cart_position',\n",
    "    # 'up_median_cart_position',\n",
    "    'days_since_prior_order_mean',\n",
    "    # 'days_since_prior_order_median',\n",
    "    'order_dow_mean',\n",
    "    # 'order_dow_median',\n",
    "    #                      'order_hour_of_day_mean',\n",
    "    # 'order_hour_of_day_median'\n",
    "]\n",
    "categories = ['product_id', 'aisle_id', 'department_id', 'user_id', 'order_id']\n",
    "features.extend(embedings)\n",
    "cat_features = [len(features) + i for i, col in enumerate(categories)]\n",
    "cat_features_array_str = list(map(lambda x: str(x), cat_features))\n",
    "cat_features_str = ','.join([str(x) for x in cat_features])\n",
    "features.extend(categories)\n",
    "\n",
    "print('categories: ', categories)\n",
    "print('cat features:', list(zip(cat_features, categories)))\n",
    "print('cat str features:', cat_features_str)\n",
    "print('not included:', set(order_train.columns.tolist()) - set(features))\n",
    "\n",
    "data = order_train[features]\n",
    "data.columns = list(map(lambda x: str(x), data.columns))\n",
    "labels = order_train[['reordered']].values.astype(np.float32).flatten()\n",
    "\n",
    "data_val = order_test[features]\n",
    "data.columns = list(map(lambda x: str(x), data.columns))\n",
    "labels_val = order_test[['reordered']].values.astype(np.float32).flatten()\n",
    "\n",
    "features = list(map(lambda x: str(x), features))\n",
    "data.to_csv(DATA_DIR + 'data_train_n.csv')\n",
    "pkl.dump(labels, open(DATA_DIR + 'labels_train_n.csv', 'wb'))\n",
    "pkl.dump(labels_val, open(DATA_DIR + 'labels_validation.csv', 'wb'))\n",
    "data_val.to_csv(DATA_DIR + 'data_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: Index(['user_product_reordered_ratio', 'reordered_sum',\n",
      "       'add_to_cart_order_inverted_mean', 'add_to_cart_order_relative_mean',\n",
      "       'reorder_prob', 'last', 'prev1', 'prev2', 'median', 'mean',\n",
      "       'dep_reordered_ratio', 'aisle_reordered_ratio', 'aisle_products',\n",
      "       'aisle_reordered', 'dep_products', 'dep_reordered', 'prod_users_unq',\n",
      "       'prod_users_unq_reordered', 'order_number', 'prod_add_to_card_mean',\n",
      "       'days_since_prior_order', 'order_dow', 'order_hour_of_day',\n",
      "       'reorder_ration', 'user_orders', 'user_order_starts_at',\n",
      "       'user_mean_days_since_prior_order', 'user_average_basket',\n",
      "       'user_distinct_products', 'user_reorder_ratio', 'user_total_products',\n",
      "       'prod_orders', 'prod_reorders', 'up_order_rate',\n",
      "       'up_orders_since_last_order', 'up_order_rate_since_first_order',\n",
      "       'up_orders', 'up_first_order', 'up_last_order', 'up_mean_cart_position',\n",
      "       'days_since_prior_order_mean', 'order_dow_mean', '0', '1', '2', '3',\n",
      "       '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16',\n",
      "       '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28',\n",
      "       '29', '30', '31', 'product_id', 'aisle_id', 'department_id', 'user_id',\n",
      "       'order_id'],\n",
      "      dtype='object'), length: 79\n",
      "\n",
      "categorical features: ['product_id', 'aisle_id', 'department_id', 'user_id', 'order_id']\n",
      "\n",
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orz/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/Users/orz/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 1.11022e-15\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[2]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[3]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[4]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[5]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[6]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[7]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[8]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[9]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[10]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[11]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[12]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[13]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[14]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[15]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[16]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[17]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[18]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[19]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[20]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[21]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[22]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[23]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[24]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[25]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[26]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[27]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[28]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[29]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[30]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "[31]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 0\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 1\tvalid_0's binary_logloss: 1.11022e-15\n",
      "Feature names: ['user_product_reordered_ratio', 'reordered_sum', 'add_to_cart_order_inverted_mean', 'add_to_cart_order_relative_mean', 'reorder_prob', 'last', 'prev1', 'prev2', 'median', 'mean', 'dep_reordered_ratio', 'aisle_reordered_ratio', 'aisle_products', 'aisle_reordered', 'dep_products', 'dep_reordered', 'prod_users_unq', 'prod_users_unq_reordered', 'order_number', 'prod_add_to_card_mean', 'days_since_prior_order', 'order_dow', 'order_hour_of_day', 'reorder_ration', 'user_orders', 'user_order_starts_at', 'user_mean_days_since_prior_order', 'user_average_basket', 'user_distinct_products', 'user_reorder_ratio', 'user_total_products', 'prod_orders', 'prod_reorders', 'up_order_rate', 'up_orders_since_last_order', 'up_order_rate_since_first_order', 'up_orders', 'up_first_order', 'up_last_order', 'up_mean_cart_position', 'days_since_prior_order_mean', 'order_dow_mean', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', 'product_id', 'aisle_id', 'department_id', 'user_id', 'order_id'] \n",
      "\n",
      "Calculating feature importance\n",
      "                             feature  importances\n",
      "0       user_product_reordered_ratio            0\n",
      "56                                14            0\n",
      "55                                13            0\n",
      "54                                12            0\n",
      "53                                11            0\n",
      "52                                10            0\n",
      "51                                 9            0\n",
      "50                                 8            0\n",
      "57                                15            0\n",
      "49                                 7            0\n",
      "47                                 5            0\n",
      "46                                 4            0\n",
      "45                                 3            0\n",
      "44                                 2            0\n",
      "43                                 1            0\n",
      "42                                 0            0\n",
      "41                    order_dow_mean            0\n",
      "48                                 6            0\n",
      "40       days_since_prior_order_mean            0\n",
      "58                                16            0\n",
      "60                                18            0\n",
      "76                     department_id            0\n",
      "75                          aisle_id            0\n",
      "74                        product_id            0\n",
      "73                                31            0\n",
      "72                                30            0\n",
      "71                                29            0\n",
      "70                                28            0\n",
      "59                                17            0\n",
      "69                                27            0\n",
      "..                               ...          ...\n",
      "9                               mean            0\n",
      "7                              prev2            0\n",
      "6                              prev1            0\n",
      "5                               last            0\n",
      "4                       reorder_prob            0\n",
      "3    add_to_cart_order_relative_mean            0\n",
      "2    add_to_cart_order_inverted_mean            0\n",
      "1                      reordered_sum            0\n",
      "8                             median            0\n",
      "38                     up_last_order            0\n",
      "18                      order_number            0\n",
      "20            days_since_prior_order            0\n",
      "36                         up_orders            0\n",
      "35   up_order_rate_since_first_order            0\n",
      "34        up_orders_since_last_order            0\n",
      "33                     up_order_rate            0\n",
      "32                     prod_reorders            0\n",
      "31                       prod_orders            0\n",
      "30               user_total_products            0\n",
      "19             prod_add_to_card_mean            0\n",
      "29                user_reorder_ratio            0\n",
      "27               user_average_basket            0\n",
      "26  user_mean_days_since_prior_order            0\n",
      "25              user_order_starts_at            0\n",
      "24                       user_orders            0\n",
      "23                    reorder_ration            0\n",
      "22                 order_hour_of_day            0\n",
      "21                         order_dow            0\n",
      "28            user_distinct_products            0\n",
      "78                          order_id            0\n",
      "\n",
      "[79 rows x 2 columns]\n",
      "Saving model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dump_model() got multiple values for argument 'num_iteration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bfe1cf0ea6b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELS_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'lgb_feature_importance.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELS_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'lgb.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELS_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'lgb_json.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELS_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'lgb_pickle.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dump_model() got multiple values for argument 'num_iteration'"
     ]
    }
   ],
   "source": [
    "print('features: {}, length: {}\\n'.format(data.columns, len(data.columns)))\n",
    "print('categorical features: {}\\n'.format(categories))\n",
    "lgb_train = lgb.Dataset(data, labels, feature_name=features, categorical_feature=categories)\n",
    "lgb_eval = lgb.Dataset(data_val, labels_val, reference=lgb_train, feature_name=features, categorical_feature=categories)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': ['binary_logloss', 'auc'],\n",
    "    'num_leaves': 256,\n",
    "    'min_sum_hessian_in_leaf': 20,\n",
    "    'max_depth': 12,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.6,\n",
    "    'verbose': 1,\n",
    "}\n",
    "\n",
    "print('Start Training')\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=2000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=30)\n",
    "print('Feature names:', gbm.feature_name(), '\\n')\n",
    "\n",
    "print('Calculating feature importance')\n",
    "df = pd.DataFrame({'feature': gbm.feature_name(),\n",
    "                   'importances': gbm.feature_importance()})\n",
    "print(df.sort_values('importances'))\n",
    "\n",
    "print('Saving model')\n",
    "df.to_csv(open(MODELS_DIR + 'lgb_feature_importance.csv', 'w'))\n",
    "gbm.save_model(MODELS_DIR + 'lgb.model', num_iteration=-1)\n",
    "gbm.dump_model(MODELS_DIR + 'lgb_json.model', num_iteration=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.Booster(model_file=MODELS_DIR + 'lgb.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
