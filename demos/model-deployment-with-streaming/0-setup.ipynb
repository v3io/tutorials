{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an MLRun project and the streams relevant for our scenario.\n",
    "\n",
    "![Model deployment with streaming Real-time operational Pipeline](../../assets/images/model-deployment-with-streaming.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-mlrun-install\"></a>The tutorial uses MLRun to create a project, implement and execute an ML pipeline, and track the execution.\n",
    "(For more information about MLRun, see Step 1.)\n",
    "To use MLRun, you must first ensure that it's installed and running as a service on your platform cluster.\n",
    "Look for an `mlrun` service on the **Services** page of the platform dashboard.\n",
    "For more information and additional assistance, contact the Iguazio [support team](mailto:support@iguazio.com).\n",
    "\n",
    "To use MLRun from Jupyter Notebook, you need to run the following code to install the `mlrun` Python package.\n",
    "This needs to be done only once per Jupyter Notebook service.\n",
    "> **Note:** You must **restart the Jupyter kernel** to complete the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already installed: mlrun\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import IPython\n",
    "\n",
    "required = {'mlrun'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "previously_installed = required.intersection(installed)\n",
    "\n",
    "if missing:\n",
    "    print(f'Installing {\",\".join(missing)}')\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "    print('Restarting kernel')\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel\n",
    "if previously_installed:\n",
    "    print(f'Already installed: {\",\".join(previously_installed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration below is shared across the notebooks. Change the values in this subsection if you would like different configuration settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projects in the platform are used to package multiple functions, workflows, and artifacts. Set here the project base name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_BASE_NAME = \"model-deployment-with-streaming\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data in the platform is stored in user-defined data containers. In this case we use the predefined \"users\" container. For more information refer to [Data containers, collections, and objects documentation](https://www.iguazio.com/docs/latest-release/concepts/containers-collections-objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER = 'users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data path where to store stream data and kv tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv, path\n",
    "\n",
    "V3IO_USERNAME = getenv('V3IO_USERNAME')\n",
    "DATA_PATH = path.join(V3IO_USERNAME, 'examples',PROJECT_BASE_NAME, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the different stream information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "WEB_API = \"http://v3io-webapi:8081\"\n",
    "WEB_API_USERS = urljoin(WEB_API, CONTAINER)\n",
    "STREAM_CONFIGS = {'generated-stream': {\n",
    "                        'path': path.join(DATA_PATH, 'generated-stream'),\n",
    "                        'shard_count': 8},\n",
    "                  'incoming-events-stream': {\n",
    "                        'path': path.join(DATA_PATH, 'incoming-events-stream'),\n",
    "                        'shard_count': 8\n",
    "                  },\n",
    "                  'enriched-events-stream': {\n",
    "                        'path': path.join(DATA_PATH, 'enriched-events-stream'),\n",
    "                        'shard_count': 8\n",
    "                  },\n",
    "                  'serving-stream': {\n",
    "                        'path': path.join(DATA_PATH, 'serving-stream'),\n",
    "                        'shard_count': 8\n",
    "                  },\n",
    "                  'inference-stream': {\n",
    "                        'path': path.join(DATA_PATH, 'inference-stream'),\n",
    "                        'shard_count': 8\n",
    "                  }\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we stream data, we associate the records with a specific partition key to ensure that similar records are assigned to the same shard. For more information, see the [stream sharding and partitioning description](https://www.iguazio.com/docs/latest-release/concepts/streams/#stream-sharding-and-partitioning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION_ATTR = \"user_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target path to store the raw data as parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The parquet files will be written via file mount, \n",
    "// hence we configure the path to start with '/User' which will be mounted to our home dir.\n",
    "PARQUET_TARGET_PATH = path.join(DATA_PATH.replace(V3IO_USERNAME, '/User'),  'events-pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target path to store the enrichment table (a key-value table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENRICHMENT_TABLE_PATH = path.join(DATA_PATH, 'enrichment-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target path to store the calculated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_TABLE_PATH = path.join(DATA_PATH, 'feature-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create V3IO Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataplane client you can manipulate data in the platform's multi-model data layer, including:\n",
    "* Objects\n",
    "* Key-values (NoSQL)\n",
    "* Streams\n",
    "* Containers\n",
    "\n",
    "Under the hood, the client connects through the platform's web API (https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/) and wraps each low level API with an interface. Calls are blocking, but you can use the batching interface to send multiple requests in parallel for greater performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import v3io.dataplane\n",
    "v3io_client = v3io.dataplane.Client(endpoint=WEB_API,\n",
    "                                    access_key=getenv('V3IO_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup previous streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete Stream call for stream generated-stream returned with status 204, and content: \n",
      "Delete Stream call for stream incoming-events-stream returned with status 204, and content: \n",
      "Delete Stream call for stream enriched-events-stream returned with status 204, and content: \n",
      "Delete Stream call for stream serving-stream returned with status 204, and content: \n",
      "Delete Stream call for stream inference-stream returned with status 204, and content: \n"
     ]
    }
   ],
   "source": [
    "for stream_name, stream_config in STREAM_CONFIGS.items():\n",
    "    resp = v3io_client.delete_stream(container=CONTAINER, path=stream_config['path'], \n",
    "                                     raise_for_status=v3io.dataplane.RaiseForStatus.never)\n",
    "    print(f'Delete Stream call for stream {stream_name} returned with status {resp.status_code}, and content: {resp.body.decode(\"utf-8\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create all streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iguazio/examples/model-deployment-with-streaming/data/generated-stream\n",
      "Create Stream call for stream generated-stream returned with status 204, and content: \n",
      "iguazio/examples/model-deployment-with-streaming/data/incoming-events-stream\n",
      "Create Stream call for stream incoming-events-stream returned with status 204, and content: \n",
      "iguazio/examples/model-deployment-with-streaming/data/enriched-events-stream\n",
      "Create Stream call for stream enriched-events-stream returned with status 204, and content: \n",
      "iguazio/examples/model-deployment-with-streaming/data/serving-stream\n",
      "Create Stream call for stream serving-stream returned with status 204, and content: \n",
      "iguazio/examples/model-deployment-with-streaming/data/inference-stream\n",
      "Create Stream call for stream inference-stream returned with status 204, and content: \n"
     ]
    }
   ],
   "source": [
    "for stream_name, stream_config in STREAM_CONFIGS.items():\n",
    "    print(stream_config['path'])\n",
    "    resp = v3io_client.create_stream(container=CONTAINER,\n",
    "                                     path=stream_config['path'],\n",
    "                                     shard_count=stream_config['shard_count'],\n",
    "                                     raise_for_status=v3io.dataplane.RaiseForStatus.never)\n",
    "    print(f'Create Stream call for stream {stream_name} returned with status {resp.status_code}, and content: {resp.body.decode(\"utf-8\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up MLRun Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projects are created by using the `new_project` MLRun method, which receives the following parameters:\n",
    "\n",
    "- **`name`** (Required) &mdash; the project name.\n",
    "- **`context`** &mdash; the path to a local project directory (the project's context directory).\n",
    "  The project directory contains a project-configuration file (default: **project.yaml**), which defines the project, and additional generated Python code.\n",
    "  The project file is created when you save your project (using the `save` MLRun project method), as demonstrated in Step 6.\n",
    "- **`functions`** &mdash; a list of functions objects or links to function code or objects.\n",
    "- **`init_git`** &mdash; set to `True` to perform Git initialization of the project directory (`context`).\n",
    "  > **Note:** It's customary to store project code and definitions in a Git repository.\n",
    "\n",
    "Projects are visible in the MLRun dashboard only after they're saved to the MLRun database, which happens whenever you run code for a project.\n",
    "\n",
    "The following code creates a project using the `PROJECT_BASE_NAME`, concatenated with your current running username in the platform (**&lt;V3IO_USERNAME&gt;**), and sets the project directory to a **conf** directory in the current demo directory (**/User/demos/model-deployment-with-streaming/conf**).\n",
    "\n",
    "> **Note:** Platform projects are shared among all users of the parent tenant, to facilitate collaboration. Therefore,\n",
    ">\n",
    "> - Synchronize your projects execution with other users on your platform cluster, as needed, or use unique project names to avoid conflicts.\n",
    ">   You can easily change the default project name for this tutorial by changing the definition of the `PROJECT_BASE_NAME` variable, defined in the beginning of the notebook.\n",
    "> - Don't include in your project proprietary information that you don't want to expose to other users.\n",
    ">   Note that while projects are a useful tool, you can easily develop and run code in the platform without using projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /User/work/tutorials/demos/model-deployment-with-streaming/conf\n",
      "Project name: model-deployment-with-streaming-iguazio\n"
     ]
    }
   ],
   "source": [
    "from mlrun import new_project\n",
    "\n",
    "project_name = '-'.join(filter(None, [PROJECT_BASE_NAME, getenv('V3IO_USERNAME', None)]))\n",
    "project_path = path.abspath('conf')\n",
    "project = new_project(project_name, project_path, init_git=True)\n",
    "\n",
    "print(f'Project path: {project_path}\\nProject name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MLRun](https://github.com/mlrun/mlrun) is a generic and convenient mechanism for data scientists and software developers to describe and run tasks related to machine learning in various, scalable runtime environments and ML pipelines while automatically tracking executed code, metadata, inputs, and outputs.\n",
    "MLRun integrates with the Nuclio serverless framework and with the Kubeflow Pipelines framework for running ML pipelines.\n",
    "The demo uses MLRun to create a project, run Nuclio serverless functions, as well as run the model training.\n",
    "Before running your code, you need to set some MLRun configurations:\n",
    "\n",
    "- <a id=\"gs-mlrun-config-artifcats-path\"></a>**Artifacts path** &mdash; the location for storing versioned data artifacts (such as files, objects, data sets, and models) that are produced or consumed by functions, runs, and workflows.\n",
    "  The path can be defined either as a local directory path or as a URL (of the format `s3://*`, `v3io://*`, etc.).\n",
    "  You can set the artifacts path either by defining an `MLRUN_ARTIFACT_PATH` environment variable (which applies globally throughout the current environment) or as part of the MLRun configuration.\n",
    " \n",
    "  If the target directory doesn't exist, MLRun creates it.\n",
    "  You can use the notation `{{run.uid}}` in the path to signify the current run ID.\n",
    "  For pipelines, you can use the notation `{{workflow.uid}}` to signify the workflow ID.\n",
    "  This allows you to create a unique artifacts directory for each executed job or workflow.\n",
    "\n",
    "  After you run an MLRun job, the artifacts directory might contain one or more of the following directories:\n",
    " \n",
    "  - **plots** &mdash; a directory for storing images, figures, and plotlines.\n",
    "  - **models** &mdash; a directory for storing all trained models.\n",
    "  - **data** &mdash; a directory for storing any other type of data artifact, such as data sets.\n",
    "\n",
    "The following code sets the artifacts path to a **artifacts** directory within the tutorial directory (**/User/demos/model-deployment-with-streaming/artifacts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts path: /User/work/tutorials/demos/model-deployment-with-streaming/artifacts\n",
      "MLRun DB path: http://mlrun-api:8080\n"
     ]
    }
   ],
   "source": [
    "from mlrun import mlconf\n",
    "\n",
    "# Target location for storing pipeline artifacts\n",
    "project.artifact_path = path.abspath('artifacts')\n",
    "# MLRun DB path or API service URL\n",
    "#mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "\n",
    "print(f'Artifacts path: {project.artifact_path}\\nMLRun DB path: {mlconf.dbpath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the configuration defined in this notebook in the project `params`. We will use these values in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.params['PROJECT_BASE_NAME'] = PROJECT_BASE_NAME\n",
    "project.params['STREAM_CONFIGS'] = STREAM_CONFIGS\n",
    "project.params['CONTAINER'] = CONTAINER\n",
    "project.params['WEB_API'] = WEB_API\n",
    "project.params['WEB_API_USERS'] = WEB_API_USERS\n",
    "project.params['PARTITION_ATTR'] = PARTITION_ATTR\n",
    "project.params['PARQUET_TARGET_PATH'] = PARQUET_TARGET_PATH\n",
    "project.params['ENRICHMENT_TABLE_PATH'] = ENRICHMENT_TABLE_PATH\n",
    "project.params['FEATURE_TABLE_PATH'] = FEATURE_TABLE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "CONTAINER": "users",
       "ENRICHMENT_TABLE_PATH": "iguazio/examples/model-deployment-with-streaming/data/enrichment-table",
       "FEATURE_TABLE_PATH": "iguazio/examples/model-deployment-with-streaming/data/feature-table",
       "PARQUET_TARGET_PATH": "/User/examples/model-deployment-with-streaming/data/events-pq",
       "PARTITION_ATTR": "user_id",
       "PROJECT_BASE_NAME": "model-deployment-with-streaming",
       "STREAM_CONFIGS": {
        "enriched-events-stream": {
         "path": "iguazio/examples/model-deployment-with-streaming/data/enriched-events-stream",
         "shard_count": 8
        },
        "generated-stream": {
         "path": "iguazio/examples/model-deployment-with-streaming/data/generated-stream",
         "shard_count": 8
        },
        "incoming-events-stream": {
         "path": "iguazio/examples/model-deployment-with-streaming/data/incoming-events-stream",
         "shard_count": 8
        },
        "inference-stream": {
         "path": "iguazio/examples/model-deployment-with-streaming/data/inference-stream",
         "shard_count": 8
        },
        "serving-stream": {
         "path": "iguazio/examples/model-deployment-with-streaming/data/serving-stream",
         "shard_count": 8
        }
       },
       "WEB_API": "http://v3io-webapi:8081",
       "WEB_API_USERS": "http://v3io-webapi:8081/users"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": true,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, JSON\n",
    "display(JSON(project.params, expanded=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to [**1-event-generator.ipynb**](1-event-generator.ipynb) to generates events for the training and serving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
