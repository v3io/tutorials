{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Platform Use-Case Application Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In This Document**\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Stock Trading](#stocks-demo)\n",
    "- [Predictive Infrastructure Monitoring](#netops-demo)\n",
    "- [Image Recognition](#image-classification-demo)\n",
    "- [Natural Language Processing (NLP)](#nlp-demo)\n",
    "- [Stream Enrichment](#stream-enrich-demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview\n",
    "\n",
    "The **demos** tutorials directory contains full end-to-end use-case applications that demonstrate how to use the Iguazio Data Science Platform (\"the platform\") and related tools to address data science requirements for different industries and implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stocks-demo\"></a>\n",
    "## Smart Stock Trading\n",
    "\n",
    "The [**stocks**](stocks/read-stocks.ipynb) demo demonstrates a smart stock-trading application: \n",
    "the application reads stock-exchange data from an internet service into a time-series database (TSDB); uses Twitter to analyze the market sentiment on specific stocks, in real time; and saves the data to a platform NoSQL table that is used for generating reports and analyzing and visualization the data in a Grafana dashboard.\n",
    "\n",
    "- The stock data is read from Twitter by using the [TwythonStreamer](https://twython.readthedocs.io/en/latest/usage/streaming_api.html) Python wrapper to the Twitter Streaming API, and saved to TSDB and NoSQL tables in the platform.\n",
    "- Sentiment analysis is done by using the [TextBlob](https://textblob.readthedocs.io/) Python library for natural language processing (NLP).\n",
    "- The analyzed data is visualized as graphs in a [Grafana](https://grafana.com/grafana) dashboard, which is created from the Jupyter notebook code.\n",
    "  The data is read from both the TSDB and NoSQL stock tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"netops-demo\"></a>\n",
    "## Predictive Infrastructure Monitoring\n",
    "\n",
    "The [**netops**](netops/generator.ipynb) demo demonstrates predictive infrastructure monitoring: the application builds, trains, and deploys a machine-learning model for analyzing and predicting failure in network devices as part of a network operations (NetOps) flow.\n",
    "The goal is to identify anomalies for device metrics &mdash; such as CPU, memory consumption, or temperature &mdash; which can signify an upcoming issue or failure.\n",
    "\n",
    "- The model training is sped up by using the [Dask](https://dask.org/) Python library for parallel computing, which extends [pandas](https://pandas.pydata.org/) DataFrames.\n",
    "- The model prediction is done by using open-source Python libraries &mdash; including [scikit-learn](https://scikit-learn.org) (a.k.a. sklearn), [SciPy](https://www.scipy.org/scipylib/), and [NumPy](http://www.numpy.org/) &mdash; and the gradient boosting ML technique.\n",
    "- The data is generated by using an open-source generator tool that was written by Iguazio.\n",
    "  This generator enables users to customize the metrics, data range, and many other parameters, and prepare a data set that's suitable for other similar use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"image-classification-demo\"></a>\n",
    "## Image Recognition\n",
    "\n",
    "The [**image-classification**](image-classification/keras-cnn-dog-or-cat-classification.ipynb) demo demonstrates image recognition: the application builds and trains an ML model that identifies (recognizes) and classifies images.\n",
    "\n",
    "- The data is collected by downloading images of dogs and cats from the Iguazio sample data-set AWS bucket.\n",
    "- The training data for the ML model is prepared by using [pandas](https://pandas.pydata.org/) DataFrames to build a predecition map.\n",
    "  The data is visualized by using the [Matplotlib](https://matplotlib.org/) Python library.\n",
    "- An image recognition and classification ML model that identifies the animal type is built and trained by using [Keras](https://keras.io/), [TensorFlow](https://www.tensorflow.org/), and [scikit-learn](https://scikit-learn.org) (a.k.a. sklearn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nlp-demo\"></a>\n",
    "## Natural Language Processing (NLP)\n",
    "\n",
    "The [**nlp**](nlp/nlp-example.ipynb) demo demonstrates natural language processing (NLP): the application processes natural-language textual data &mdash; including spelling correction and sentiment analysis &mdash; and generates a Nuclio serverless function that translates any given text string to another (configurable) language.\n",
    "\n",
    "- The textual data is collected and processed by using the [TextBlob](https://textblob.readthedocs.io/) Python NLP library. The processing includes spelling correction and sentiment analysis.\n",
    "- A serverless function that translates text to another language, which is configured in an environment variable, is generated by using the [Nuclio](https://nuclio.io/) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stream-enrich-demo\"></a>\n",
    "### Stream Enrichment\n",
    "\n",
    "The [**stream-enrich**](stream-enrich/stream-enrich.ipynb) demo demonstrates a typical stream-based data-engineering pipeline, which is required in many real-world scenarios: data is streamed from an event streaming engine; the data is enriched, in real time, using data from a NoSQL table; the enriched data is saved to an output data stream and then consumed from this stream.\n",
    "\n",
    "- Car-owner data is streamed into the platform from a simulated streaming engine by using an event-triggered [Nuclio](https://nuclio.io/) serverless function.\n",
    "- The data is written (ingested) into an input platform stream by using the the platform's [Streaming Web API](https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/streaming-web-api/).\n",
    "- The input stream data is enriched with additional data, such as the car's color and vendor, and the enriched data is saved to a NoSQL table by using the platform's [NoSQL Web API](https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/nosql-web-api/).\n",
    "- The Nuclio function writes the enriched data to an output platform data stream by using the platform's Streaming Web API.\n",
    "- The enriched data is read (consumed) from the output stream by using the platform's Streaming Web API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
